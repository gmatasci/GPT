# Dataset
csv_filepath: interview_data.csv
characters_to_drop:
    - "-"

inspect_data: false

train_test_pct: 0.8
train_val_pct: 0.8

# Data config
tokenizer_name: character  # cl100k_base (for gpt-4), gpt2, character
# tokenizer_name: cl100k_base

# Model config
#model_name: bigram  # bigram, gpt
model_name: gpt

n_batches_to_print: 1000
#max_iters: null  # 500
max_iters: 5000

# Hyperparameters
n_epochs: 5
learning_rate: 0.0001

batch_size: 256  # how many independent sequences will we process in parallel?
#batch_size: 4
block_size: 256  # what is the maximum context length for predictions?
#block_size: 8
n_embd: 384
n_head: 6
n_transformer_blocks: 6
dropout: 0.2